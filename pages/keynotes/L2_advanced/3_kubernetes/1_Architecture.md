---
title: kubernetes架构
keywords: keynotes, L2_advanced, 1_kubernetes, 4_learn_kubernetes, 1_Architecture
permalink: keynotes_L2_advanced_1_kubernetes_4_learn_kubernetes_1_Architecture.html
sidebar: keynotes_L2_advanced_sidebar
typora-copy-images-to: ./pics/1_Architecture
typora-root-url: ../../../../../cloudnative365.github.io

---

## 前言：容器编排之争

### 1. Docker三剑客

我们以往在实现安装部署应用程序的时候，像nginx，mysql，或者一个网站架构，手动去做是非常繁琐的。所以我们后来有了运维工具，Ansible，puppet，而ansible这种工具其实就是一种应用编排工具，服务的安装，配置，启动，通过我们定义的playbook，还有一些参数的定义，逻辑的处理，完成对多种应用程序的组合部署。从而完成了我们本来需要人手工操作的一系列任务。他能够把我们本来应该手动执行的应用编排成为任务，通过另外一个工具，或者是应用程序，把我们日常的任务注入到当中去，这就是Ansible或者是同类的编排工具，比如saltstack等一众工具要完成的功能。

直到Docker的出现。以往，我们手工管理的对象是直接部署在操作系统环境当中的应用程序，但是有了Docker之后，我们的应用程序可以容器化，各种应用程序都被封装在了容器中执行。想象一下，我们最早在使用Ansible编排应用程序的过程当中，我们经常会发现对象在不停的变化，这个时候，我们的ansible或者puppet这种传统的应用程序或者工具已经不再适合这种场景了。因为，容器化所提供的接口，与我们早期传统形式上的应用程序的访问控制和管理是有所不同的。所以，在docker时代就开始呼唤新式的，面向容器化应用的编排工具。

在docker出现以后，随之而来的编排工具立即就出现了，早期，我们接触的有三款产品。第一款是Docker自身提供的，叫docker compose，这个工具更适合于单机编排。他只能面向一个docker hosts来进行编排操作。或者是更加适用于这种情景。Docker就不得不开始提供一款工具能够将多主机的Docker host，也就是很多的docker主机，能有一定程度上的调度效果的，至少是面向集群的编排效果。那于是Docker后来提供了另外一款工具，叫docker swarm，而这个Docker swarm就是一个能够将多个docker整合成为由一个统一的管理工具管理之下的集群的工具，我们将多个host所提供的计算资源整合在一起，整合成为一个资源池，随后docker compose在编排时，只需要针对Docker swarm整合出来的资源池进行编排就可以了，而不用关心底层是什么样的主机，或者有几个docker主机。当然，docker swarm也是一个应用程序，甚至可以理解成为Docker自身的应用程序，那么，这里面的主机，怎样成为docker主机的呢？一个主机，要想加入docker swarm资源池，成为docker swarm的成员，他自己首先是一个docker 主机，那么docker 如何安装上去，他怎样加入docker swarm集群，成为docker swarm资源池的一员呢？我们就需要另外的工具，叫docker machine，能够将一个主机迅速初始化，满足加入docker swarm的集群的先决条件，从而能够成为docker swarm集群的一份子，这样一个预置处理工具，这就是人们当年口中的docker编排三剑客。我们知道，此前docker compose 面向单个的docker host，照样可以工作，当时的人们就是这么编排工具。

我们可以看到，人们不再是面向单个主机上的应用程序去编排，就好像ansible，他本来就可以面向多台主机上的应用程序去编排，那么，容器化的编排工具也需要面向多级执行编排，而且更重要的是，我们将来要运行容器时，在不同主机上编排时候，可以会用到同一个应用程序，这同一个应用程序，如果编排在不同主机上不相冲突等各种任务，都需要有Docker编排工具，或者容器编排工具来实现。

![image-20200307231554527](/pages/keynotes/L2_advanced/1_kubernetes/pics/1_Architecture/image-20200307231554527.png)

### 2. Mesos

那么，除了docker自身所提供的工具，我们还有第二类工具，mesos，目前是apache旗下的产品。他原来是由Twitter受到google borg系统的启发，想开发一款资源管理的软件。整个时候，他们发现加州大学博客利分校AMPLab正在开发一款叫做mesos的软件，后来他们就把负责开发这款软件的人挖到了Twitter，然后开始开发和部署mesos。但是mesos是一个面向IDC的OS，IDC的操作系统能够吧IDC当中所有的硬件所提供的计算资源统一的调度和分配，但是他所面向的上层接口，提供的不是容器运行的接口，而只是资源分配工具，并非能够运行和托管容器的，所以在此之上，他必须还要提供一个能够允许容器编排的框架，叫marathon。

### 3. Kuberntes

第三个呢，就是kubernetes了，早期有这三款常见的工具。

当然，后来的事实是，kubernetes在这个领域中据说已经牢牢占据了80%以上的份额。一个产品能够占据35%以上的市场份额就已经是自然垄断了。他的量级就已经能够揣测出来了。

---

### 4. DevOps

而在容器编排之外，我们在IT的工作领域当中，还涉及到另外一些概念，像DevOps，Microservice，容器，云计算，blockchain各种概念层出不穷，处于业务本身迭代的需要，我们对应的开发模式也在不停的发生变化，比如早期的瀑布式，到后期的敏捷开发，在到后来做精益开发，到现在，我们把QA，operations，以及开发，整合起来形成devops。这种所谓应用模式的开发，而应用程序的架构，从早期的单体架构，我们知道，一个应用，吧所有的功能都做在一个软件程序当中，早期的应用复杂程序并没有这么高，所以我们的程序作为单体是没有问题的，但是后来，人们发现单体应用程序难以承载的原因是，单个的应用程序只能装在单个的主机上，而随着业务的扩展就很容器导致内部应用的扩展达到硬件资源的上限，于是我们就把单体给分开，形成分层架构，amp（apache，mysql，php），在往后就是微服务。到目前为止，我们不再是简单的分层，而是把每一层的应用都拆解成一个微小的服务，每个服务都只干一件事，而后呢，而我们原来认为一个单体应用可能会拆成100个甚至拆解成数百个微小的服务，让他们进行彼此之间的协作。那么，微服务之间的调用关系就变得极其复杂了，谁该调用谁，如何确保调用者和被调用者是始终存在的。因此，目前很多微服务都是架构在容器之上的。利用容器本身的特性。我们讲过很多容器的优势，比如分发构建和部署都是非常方便的，所以利用这些特性，使得微服务和对应的容器技术结合起来以后迅速找到了一个让自己落地的实现方案。devops也是如此，因为docker技术的出现，使得早期的devops技术中的交付和部署环节，由于异构的原因，导致部署起来非常困难，恰恰容器的出现弥补了这个裂缝，使得devops非常容易实现。

---

这里简单介绍一个Devops中的几个术语，我们后面可能会用到，一个叫CI/持续集成，一个叫CD/持续交付delivery，还有一个CD叫持续部署，deployment。持续集成我们就可以简单理解为，当我们有一个应用程序，我们要将他发布到线上，而发布到线上的做法，我们之前见过的，发布的模型当中，有蓝绿部署，灰度/金丝雀部署，那么有没有想过这个应用程序交付到我们运维人员手中，他们经历了什么？我们就理解为我们有一个开发团队，他们在开发之前要做plan，然后做架构设计，然后开始做开发，然后构建，我们有的时候需要把构建好的软件发布到真正的环境当中才可以进行后面的步骤，就是做测试，各种测试，测试如果有问题，就需要重新修复，如果充分测试没有问题，就该给运维人员了，在交付给运维之前，如果能够持续的，利用一些工具，自动的实现。到目前为止，我们这些过程当中，开发一般都是人来做的，由程序猿来做，而构建的过程，我们可以用构建工具来实现，就好像我们讲过的运维在拿到一些源代码之后，想要使用，就需要先编译，这个编译就是一个构建的过程，当然，它的背后的一系列过程，编译，链接，构建，当然这是C语言的过程，而java，就需要maven，gridle之类的工具来做。假如说，当程序猿在往后的开发完代码之后，这些步骤能够自动实现，就叫做持续集成，比如：我们本地的程序猿，开发完一段内容，在本地做完代码，并且提交之后，代码会自动提交到代码仓库当中，我们有一款工具就是构建工具他能够触发创建操作，当用户提交完代码之后，这个工具能够吧代码拖出来做构建。构建完成之后，能够吧程序部署到测试环境中，自动进行测试。这些测试都没问题之后，就进入下一步。有问题，还要通知程序猿进行修改，就这么不断的提交，打包，构建，部署，测试。在这个过程中任何一个部分出问题，都会通知程序猿让他重新提交，我们称这个过程就是持续集成的过程，而这个过程中，需要人参与的就是一个，就是程序开发。但是对于不同的程序来讲，他的测试方法一定是不一样的。因此需要对自己的应用进行全方位的测试的时候，也会开发一些自动化测试工具。我们还会有测试开发工程师来开发测试专用的工具，有一些程序就可以使用市面上通用的测试软件，通过编写一些测试代码来实现。 持续集成完成之后，就需要交付给运维去部署了。如果我们测试完成之后，软件会被打包到一个运维和客户可以得到的，共享的服务上，或者是一个仓库当中，我们的运维工程师能够得到打包好的，构建好的最终的运维产品，吧这步也能自动实现，这个就叫做持续交付，假如交付之后，我们的部署工作也不需要运维工程师来做了，一旦交付完成之后，我们有一款工具，能够自动吧交付物拖出来，吧他自动触发以后，发布到线上，这个就叫做持续部署。而这一切能够自动循环，实现持续的过程。就叫做devops。我们在持续循环的过程当中，可能会发现此前未发现的bug，吧这个信息快速反馈给信息，快速修复，快速的发布，在上线新版本。周而复始的形成了devops，而这就打通了开发，测试和运维的边界，而这个时候，运维就容易被干掉了。以后在这个的循环当中，每个环节的工具，就是我们运维工程师要关注的项目。



而恰恰就是容器技术的出现，和容器编排工具的实现，使得这一切容器落地，原因在于我们构建好的产品要部署在目标平台上，而平台有可能不仅是我们的平台，而是客户的平台，就好像一些开源软件一样，客户需要下载，并且部署在他的平台之上，不同的客户不同的环境，特别是异构环境。就算是同一个平台，也有不同的版本，这就使得我们构建时，我们的构建目标就很广，因此在实现自动构建和自动交付的时候，这个环节就非常困难，我们为了每一种目标环境，就不得不构建出一个适用的版本来。但是有了容器之后，这个就不是问题了，只要你的目标平台是能够支持容器的，我们构建的时候就构建成镜像，不管是什么平台，就把镜像运行为容器就好了。所以使得自动部署这种非常困难的事情就变得非常容易。正是容器技术的出现使得容器技术在devops技术落地上，完全实现了可能。



但是以后，如果我们把大量的应用都构建为容器的话，很显然他不可能运行在单个主机上，我们运行在多个主机上的时候，那些容器需要面向内部通讯，那些容器需要对外进行通讯，他们要怎样编排，谁和谁应该有依赖，这都是非常麻烦的事，这对于人来说几乎是无法完成的任务，如果我们在走向微服务化之后，有200个服务，服务出故障是必然的，每天都有可能几个，或者几十个的出故障，靠人来修复是完全来不及的。而且内部的复杂度，靠人的脑子去梳理，几乎是无法完成的任务，因此，容器化时代，我们手工去管理容器，几乎是不可能完成的任务。我们必须要使用容器编排工具来实现。



而这些技术就可以融合在一起来解决这些问题，正是这些技术，让我们呼唤已久的devops技术得以落地。而后，devops技术落地之后，又完全的和容器化技术相结合，而容器编排又被进一步需要和依赖。虽然devops技术不一定要依托于容器之上，但是恰恰是容器这种技术的出现，使得devops得以迅速流行，而又是devops的流行，又使得我们的容器编排成为一款重要的底层技术工具。所以我们现在很多的devops环境都是构建在容器编排环境当中的。



但是容器编排工具自身，并不能提供devops环境，各位需要在掌握了容器编排工具之后，吧devops这种技术，这种文化构建，落地于容器编排工具之上。Devops本身并不是一种技术，他是一种文化，是一种运动，是一种趋势，针对我们手工操作的这种工作模式，用工具化的方式来解决，来突破Dev和ops的一种工作屏障，让他们可以协同起来进行工作。

## 第一章 k8s

k8s出现的时间并不是很长，只有区区几年的时间，kubernetes中文的意思叫做舵手，而他的标志就是一个舵，或者叫飞行员。他主要是由Google的几位工程师创立的，大概是在2014年对外首次宣布，一直到今天为止，也不过是短短6年的时间，kubernetes的开发，深受google内部的borg系统的影响。Borg系统是google内部的，已经工作了10几年的，非常稳定的容器编排工具。Borg是google内部使用的大规模的集群管理系统，久负盛名，人们对他觊觎已久，但是没有渠道获得的话，就开始开发自己的产品。当谷歌的攻城狮发现这块市场要被占领的时候，就用go语言重构了borg系统，沿用思路而不沿用代码的方式来进行构建。短短几年时间，kubernetes系统发展非常的迅速，尤其是国内对于kubernetes这种新兴的技术倾注了大量的心血和精力，在相关项目的参与度非常高。也正是kubernetes是站在borg系统的肩膀上，他从一出世开始就吸引了太多的目光和关注，到现在为止，他也确实没有辜负人们的期望。

他在希腊语的意思里面是舵手的意思，所以他的标识也是个舵，kubernetes的1.0版本在2015年7月份才发布，到今天为止，它的版本已经到了1.18版本，kubenetes的代码托管在github之上（看github仓库）www.github.com/kubernetes/kubernetes，他的账号叫kuberntes，第二个kuberntes是软件的名字，看release，可以看到目前正在维护的版本，差不多是每年4个版本的速度在迭代。而2017年在容器的发展史上是非常具有里程碑意义的一年，因为在这一年当中AWS，还有微软的Azure，还有国内的著名的阿里云，这些著名的云计算公司都宣布在他们的平台上原生支持k8s，他允许用户点击几个按钮就可以快速部署k8s应用，这种就是面向kuberntes的云原生，还有一些平台甚至可以吧自己的k8s直接提供给用户，让客户可以直接把应用部署在上面，也就是容器即服务的环境。也正是因为这些大型云厂商的支持，也使得K8s在业内收到了广泛的认可和支持，大概在2017年10月份，docker宣布他们的docker swarm上也支持k8s和docker compose，我们知道docker swarm本来是docker公司用来占领容器编排市场的最有力武器，但是docker在他的企业版本的发行过程当中，同时支持swarm和kuberntes，本来是竞争对手的，但是后来不得不把他整合进自己的产品当中去，才能获得竞争优势，同时rkt也是另外一款容器工具，rkt的容器工具当中，也有自己的编排工具叫fleet，但是他也放弃了这个项目，而原生使用kubernetes，rkt是coreos旗下的产品，而coreos目前已经被红帽收购了，这种大价钱还表现在红帽在他的PaaS产品openshift的核心就是kubernetes，但是kubernetes只是一个容器编排工具，没有到达PaaS这种平台即服务的标准，但是openshift是其中一个实现，我们也可以认为openshift是kuberntes的发行版。我们知道，kubernetes做的非常底层，真正离终端用户要自己使用kubernetes，你需要在kubernetes上面部署很多工具，以满足devops的需要，或者满足自己PaaS的需要，那么openshifit就是一个解决方法，他的里面已经集成了一切关于devops和PaaS所需要的一起工具。红帽又被IBM收购了，那么openstack这款PaaS就属于IBM了，我们可以看到这些大厂商已经花了大价钱在容器编排市场上



那么说一下kubernetes的特性：

首先，他可以实现自动装箱，基于资源依赖及其他约束，能够自动完成容器的部署，而不影响其可用性

可以自我修复，有自愈能力，

自动实现水平扩展，一个不够就在启动一个，再不够就在启动一个，只要你物理平台的支撑是足够的。

还可以自动实现服务发现和负载均衡，因此，当我们在k8s上有很多应用程序的时候，程序和程序之间如果存在依赖，他就可以通过服务发现的方式，找到依赖的服务，如果每个服务，我们启动了多个容器，他可以实现自动的负载均衡。

可以自动的发布和回滚。

还支持秘钥和配置管理。对于大量的服务来说，我们配置容器内部的应用比较繁琐，如果我们期望容器内部的镜像换一个配置，我们怎么换呢？如果单纯使用容器，我们就需要重新定义容器的entry，我们需要传递给entry一些值，让他变成内部应用程序可读取的配置信息，从而完成容器的配置。之所以这么麻烦，是在于早期的应用程序并不是面向云原生而开发的。所以那些应用程序通过读取参数来完成配置，而云原生的应用程序最好是基于环境变量来获取变量，也就是容器启动之后，可以直接通过环境变量来获取你传递给容器的一些值，但是传统的非云原生运行环境，我们也需要让他们通过环境变量来配置的怎么办呢，我们就需要使用entry point脚本，我们可以使用sed或者echo这样的语句，让用户吧传递过来的环境变量的值替换到这个应用程序的配置文件中，可以使得容器启动，应用程序加载环境变量时，可以读取到用户通过环境变量传递过来的配置信息。但是这种方法，不能满足用户在不同环境下运行同一个镜像，在不同配置的情况下的这么一个需求，我们当然也可是使用镜像的方式，但是这种方式是一定弊端的，比如，你的配置信息要保存在哪。我们容器启动之后需要手工传递环境变量的值。所以我们就需要一个外部的组件，来自动的保存这些配置信息，当镜像启动时，我们让容器加载外部配置中心当中的配置信息就可以启动。配置中心就是指在大规模的应用环境当中。比如，你有20个NGINX服务器，这20个NGINX就是我们水平扩展之后用来解决负载均衡的upstream，如果我们修改了某一个配置，我们就需要使用ansible这样的配置工具，吧配置推送到每一个服务器上，还需要让每一个服务生效。这个是需要手动推过去的。有一种简单的方式，我们让应用程序加载的配置信息放在一个服务器上，应用程序启动的时候不是通过加载文件来获取配置信息的，而是加载服务器上提供的配置信息。以后我们要改配置文件，就直接到配置中心吧配置文件改掉就好了。告诉这些应用，直接加载新的配置信息就好了。想这种需求可以在k8s上轻松实现，我们可以吧配置信息保存成k8s的一个对象，每一个用到此配置对象的容器启动时候，直接加载，

另外k8s还能实现存储编排，吧存储卷实现动态编辑，也就意味着某一个容器需要用到存储卷时，根据容器自身的需要，创建能够满足他需要的存储卷，实现存储编排

另外，还能实现批量处理运行。这些特性是kubernetes官方给出的特性说明。



环境架构，概念和术语



![img](/pages/keynotes/L2_advanced/1_kubernetes/pics/1_Architecture/2cccb2314ff6484198580d08119cc78e.jpeg)

kubernetes从运维的角度来说，其实就是一个集群，组合多台主机整合成为一个大的资源池，并统一对外提供计算，存储等能力的集群，这个集群，就是找几台主机，每台主机都安装上相关的kubernetes应用程序，并通过这个应用程序协同工作，吧多个主机当一个主机来使用。前提就是，我们需要在每一个主机上安装上相关的应用程序。让大家在这个级别的程序上完成通讯，从而形成彼此间的协调，



但是，在k8s的集群当中，主机是分角色的。一般来说，集群有所谓的两种场景模型，一种是p2p的，比如redis的cluster，没有中心节点， 每一个节点都可以接受请求，第二种就是有中心节点的，比如mysql的主从复制，一个节点是主节点，其他的都和主节点同步。在分布式系统当中，比较著名的就是hdfs这种文件系统，他们也是有中心节点的，他叫做名称节点。那么k8s就是一个有中心节点的系统架构的系统，他是master/nodes模型，他有一组节点来做master，一组节点做node节点。一般的master有三个就足够了，而nodes或者worker，负责工作，从这个角度来讲，master更像是蜂王，nodes就是工蜂。

有一个或者一组节点是主节点，一般来说是三个，他们互相来做高可用。而后各node节点都是一些贡献存储能力，计算能力等相关能力的资源节点。他们就是为了运行容器的节点。那么用户怎么在集群中运行程序呢？客户端的请求要发给master，这个客户端是说管理人员，创建启动容器的请求，交给master，master当中有一个调度器去分析各个node现有的可用资源的状态，找一个最佳的，适合运行用户所请求的容器的节点，并且把它调度上去，由这个node本地的docker或者其他运行环境，负责吧这个容器启动起来。启动容器是不是需要用到镜像呢？镜像在什么地方，他在registry当中。而docker在启动容器的时候，先检查本地有没有镜像，如果有镜像，就直接启动， 没有镜像就去仓库里面吧镜像拖出来，而kubernetes并不提供镜像的托管服务，docker是去其他的registry中下载的。我们是不是可以创建私有registry？那么registry是不是可以也可以是容器啊，其实我们完全可以吧registry托管在kubernetes之上，其实kubernetes其实还可以托管自身，吧kubernetes运行在kubernetes之上。我们在理解这种最简单的模型之后，我们在理解其他的模型就更加容器了，大家可以先简单理解一下概念，这就是所谓的kubernetes功能模型。



我们刚才提到，接受请求的只能是kubernetes的master，我们知道，我们向客户提供一些远程服务一般会通过套接字向外提供服务，而且套接字向外提供服务一般都是提供API，所以我们的客户端都是需要编程访问，或者一些编程出来的客户端来访问，就好像mysql，mysql server监听在3306端口上，我们怎么和他打交道呢，我们必须使用mysql客户端来访问，或者是php，jsp这种代码的方式，用驱动程序写代码，所以，从这个角度来讲，mysql提供的3306端口实际上是API接口，mysql的API服务，他是个API服务器，同样道理，kubernetes就把master上面的称为API-server，那么我们刚才说过了，API server只是用来接收请求，解析请求，至于用户这次的请求是要创建一个容器，那么这个容器不应该运行在master之上，应该运行在node之上，而哪个node更适用呢？这个是由另外一个组件，叫调度器，叫scheduler，他负责去观测每一个node之上，可用的ram和存储资源，并根据用户所请求的，创建容器所需要的资源量，CPU多少，内存多少。我们知道做docker是可以做资源限制的，需要使用多少CPU多少内存可以加一个限额。在kubernetes之上，不但能够设定资源的阈值，上限，还能设定容器资源的下限。这就是说，我们要保证节点上至少有多少资源可用。那么调度器就是根据容器资源的最低需求来进行评估的。哪一个节点最合适。但是要知道，我们资源请求的维度可不止一个，比如我们请求至少4核的CPU，那么这个指标就需要考量。这个就是调度器所需要完成的任务。为此，kubernetes为此设计了一个分级调度的方式来完成这个任务，第一步，先做预选，就是评估在所有节点当中，有多少节点是符合我们容器运行需求的，而筛选出来的节点，会再做优选，选择一个再做最佳适配。那么谁是最佳呢，这于你的调度算法当中的优选算法来决定。从而选出最佳的节点。



下面，如果我们在某一个节点上，吧某个容器启动起来了，万一这个容器中的应用程序意外崩溃的。我们提到过，容器内部是有容器的健康度检测的机制的。我们不可以简单的从程序的运行与否来决定容器的健康与否，这个是需要额外定义的，这个叫容器健康探测，我们叫他可用性探测机制，来探测服务的可用性。一旦容器中的应用挂了，但是我们又需要这个容器始终在运行状态，node之上有个应用程序，叫kubelet来负责这个程序始终处于健康运行状态，如果node节点宕机了，那么所有托管在这个节点上的node容器也就不见了。但是kubernetes是有自愈能力的，一旦容器不见了，不用人工干预，我们只需要再启动一个一模一样的来取代原来的位置就可以了。那么接下来在其他节点就创建出一个一模一样的容器来就可以了。那么如何确保这个容器始终是健康的，怎么才能知道他出故障了呢，我们就需要持续的监控他们，所以kubernetes还实现了一大堆控制器的应用程序，负责监控在他管理之下的容器是否是健康的，一旦发现不健康，就向masterserver的API发请求，说我的容器挂了一个，你帮忙在调度重新启动一个，然后master会把请求转发给scheduler，scheduler再去节点当中挑一个合适的，再启动一个，还有一个组件叫控制器，这个控制器在本地不停的loop，周期性探测，他所管理的容器是否是健康的，或者说他不符合用户所定义的目标状态，就需要确保他始终不断的移向客户所期望的状态，以确保他是符合用户期望的。那么我们有这么多容器，每一类都有一个控制器来控制。我们现在就假设说，我们现在有很多控制器，如果控制器挂了呢？因此，在master之上还有第三个组件，叫controller-manager，他负责监控着每一个控制器，负责确保每一个控制器是健康的。如果控制器管理器不健康的，因此在控制器级别做冗余。如果三个控制器管理器，那么只有一个在工作，其他都是做冗余。



pod

所以说，master节点非常重要，他是整个集群的核心，是大脑，在他的上面有三个组件，第一，API-server，负责接收和处理请求，第二，scheduler，调度容器创建的请求，第三，控制器管理器，确保已经创建的容器处于健康状态，当然，控制器管理器是负责控制器的健康的，而控制器才是用来确保容器健康的。kubernetes支持众多类型的控制器，支持容器控制器健康的只是其中一种，以后我们在说k8s上的单元的时候就不能说是容器了，因为在k8s上的最小单元是pod。kubernetes并不直接调度容器的运行，他调度的目标叫pod，pod可以是理解成是容器的外壳，对容器做了一层抽象的封装。所以pod变成k8s系统上最小的调度的逻辑单元，pod内部主要就是用来放容器的，但是pod也有几个特点。我们说pod可以将几个容器放在一起，共享一个网络名称空间。也就是说k8s做了一个逻辑组件叫pod，在pod内用来运行容器，但是一个pod可以运行多个容器。在多个容器共享同一个共同的网络名称空间。共享底层的net，uts和ipc三个网络名称空间，但是，另外三个，互相隔离。user，pid，mnt。这样一来，一个pod内的多个容器，共享主机名之类的名称空间，他更像是一个宿主机上运行了多个虚拟机。一个宿主机上运行了多个虚拟机，这个虚拟机上运行了应用程序，这些虚拟机使用同一个lo进行通讯。这就是kubernes在组织容器的时候，使用的一个非常精巧的办法，使得我们可以构建较为精细的容器间通讯了。同一个pod内的容器还共享第二个资源，存储卷，假如我们定义了一个存储卷，让第一个容器可以访问，那么，第二个容器也同样可以访问。存储卷不再属于容器，而属于pod。那么每个node主要是来运行pod，一般说来，一个pod内只放一个容器，除非容器间有非常紧密的关系，需要放在同一个pod中，另外如果需要在pod中放多个容器，通常我们称主要功能的那个叫主容器，其他辅助主容器完成其他功能的。假设，我们这个容器中跑的是nginx，他会生成很多日志，我们要收集日志，通常需要在目标服务器上部署一个日志收集agent，filebeat或者logstash，一个容器中只运行一个程序，你要是运行nginx，就不能运行日志收集程序了，我们应该吧日志收集程序放在哪呢，就放在另外一个容器当中。让他来辅助pod内的主程序所不能完成的工作，这些容器叫做sidecar。他们是用来辅助主程序中的某些功能的。



其实，官方根据辅助容器的功能不同，分了三类，但是这三类并不是说某一个参数控制的，而是根据功能分类而已。第一类，就叫sidecar，第二类，叫ambassador，有个软件就叫ambassador，最著名的，就是istio。第三类，叫Adapter。其实就是类似于媒婆。



所以我们调度器调度的是pod，pod是一个原子单元，一个pod中有一个容器或者多个容器，一旦我们把一个pod调度到某一个node上运行之后，一个pod内的所有容器都只能运行在同一个node之上，



在说说node，node是kubernetes上的工作节点，负责运行，由master指派的各种任务，而最根本的是以pod形式运行容器的。理论上讲，node可以是任何形式的计算设备，只要能够有传统意义上的内存，cpu，存储空间，并且可以安装kubernetes的集群代理程序，都可以作为整个kubernetes集群的一份子进行工作，而kubernetes所实现的效果，就类似类似这个样子，一共有xcpu yram，他是由kube_cluster来统一管理，而后我们master上就相当于拥有了这么一个池子，当用户请求创建资源时，我们到底还有多少CPU和内存，我们可以做一个统一的调度和评估。这样，终端用户就无须在关心到底是运行在哪个节点上，实现了类似于云服务。所以他的调度和编排脱离了终端用户的实现，终端用户无须在关心容器究竟运行于哪个node之上，也就实现了吧所有的节点都当做一个统一的资源池来进行统一管理，但是这里需要注意的问题是，这么多pod都运行在一个集群当中，要想分类管理怎么办，我们想要删除一类pod，我想让某个控制器只管理某一类pod，让控制器只挑选出这一类的pod呢，一般来说，不是靠名称来识别，名称不是固定的，唯一的标识符，同时我们还有可能将一类的pod归组，我们有四个nginx pod，我们想用一个控制器来管理这四个pod，我们想删除这四个pod，只需要把这四个pod的控制器删了，控制器需要保证这四个pod都是在的，精确符合四个的要求才行，因此他怎么认为符合我们期望的有几个呢？因此为了能够实现pod被识别，我们需要在pod上附加一些元数据，就好像docker 在docker file之上打上一些标签，这就是一些kv类型的数据，我们在kubernetes之上打pod的标签，跟容器还是两回事，我们用类似的效果，用标签来识别pod，也就意味着，你创建完pod之后，可以直接给pod打上标签，让控制器或者人，基于这个标签的值，识别出标签，或者识别出pod来，我们创建了4个nginx pod，我们可以给每一个pod上加一个标签叫app，app=nginx，我们想把这一类挑出来，怎么挑呢，拥有key叫app，并且这个app的值是nginx，第一，要有app这个标签，第二，app的值叫nginx，我们就可以吧他挑选出来了，这个标签实际上可以想象成一个便签。创建完pod之后就给他贴上。所以我们说标签是我们在kubernetes的管理上，可以识别pod和管理pod的一个非常重要的凭证和途径，而这个组件就叫做标签选择器，label selector，简称selector。

标签选择器就是根据条件来过滤符合条件的资源对象的机制，标签不光是pod可以用，其他很多资源都可以用。k8s是一个restful风格的API，http或者https协议对外提供服务，因此restful风格当中，几乎所有被操作的目标都是对象。所有对象都可以拥有标签，所有的对象都可以被标签选择器选择，pod就是其中最重要的一类对象。

在k8s之上，同一类的pod可能不止一个，既然如此，当用户的请求到达时，我们如何去接入用户请求，如何去对同一类pod中的哪一类pod做处理和响应呢？pod尽量使用pod控制器来管理，尽量不要手工去管理。事实上，我们管这一类叫做自主式pod，我们创建这类pod的时候，仍然是需要提交给APIserver，由APIserver接受以后，借助于调度器将其调度到指定的node节点，由node启动此pod，如果pod出现问题，需要重启容器，是由kubelet完成，但是如果节点故障，那么容器就消失不见了。所以我们建议使用第二种pod，由pod控制器控制管理的pod。正是控制器这种机制的引用，在kubernetes集群设计当中，pod可以说是一个有生命周期的对象。而后由调度器将其调度到集群当中的某个节点，如果任务终止，他就会被停掉。但是有一些任务，是作为守护进程运行的，比如nginx，这种应该时刻处在运行状态，一旦出现故障，直接就取代他，或者重启他。这一切的一切，让监控去盯着这些都不太容器实现，更别说我们用眼睛盯着了。所以，容器平台自身应该提供这种功能，而k8s为此提供的功能就叫做pod控制器。pod控制器有很多种，最早的叫replicationController，副本控制器。意思是，当我们启动一个pod时，这个pod如果不够了，就再起一个，这个叫副本，而后，控制器就专门控制着同一类pod的副本数量，一旦副本数量少了，就自动加一个，多了，就干掉一个。一定要精确符合定义的pod数量。比如，我们pod所在节点挂了，RC就会找到api-server，api-server找到scheduler在找一个地方，创建一个新的节点。这个时候，就变成了精确符合pod要求的副本数了。他还能实现其他的任务，比如滚动更新，此前启动容器。比如，此前我们启动的镜像是1.0版本的，后来有了1.1版本的镜像，需要把所有pod中的应用程序版本变成1.1的镜像，我们就需要滚动更新。我们可以创建一个新的pod，然后去掉一个老的，然后创建一个新的，在去掉一个老的，这就是滚动更新。而且他也支持回滚。早期的时候pod控制器，只有这一个RC。

到了新版本之后，又有了新的控制器，叫replicaSet，叫副本集控制器，但是replicaSet不直接使用，他有一个声明式更新控制器，deployment，所以我们用的最多的是deployment，deployment是用来管理那些无状态的应用，有状态的应用需要使用statefulset，有状态副本集，如果我们需要在每一个node上运行一个副本，而不是随意运行，叫daemonset，作业叫job，周期性运行作业cronjob，这些就是常见的pod控制器。特别是后面这些应用，是用来做特定应用管理的，job是指类似于备份这种操作，或者某一次时间不固定的操作，比如我们现在要对一大堆的数据集进行清理，那么临时启动一个pod做清理，那么清理完了之后，pod就可以结束了。这个程序运行就结束了，这种不需要一直处于运行状态的，我们就叫他job，但是这个job没执行完就挂了，我们需要把他重启起来，一旦job执行完，就不需要再启动了，但是对于nginx来说，只要用户没把控制器删了，我们需要他始终处于运行状态，这么多控制器是用来确保不同类型的pod资源，来符合用户所期望的方式来运行。



另外像deployment这种控制器还支持hpa这种二级控制器，叫水平pod自动伸缩控制器，比如，我们目前有两个pod在运行，万一用户量大了，不足以承担这个访问量怎么办呢？我们就应该加更多的pod资源了，那么到底应该加几个呢？那么hpa控制器可以自动监控，自动扩展。比如：他会监控现在pod的CPU和内存利用率是多高，我们需要确保他平均低于60%，如果一算，还需要两个pod才能保证，那么就加上两个。一旦访问量小了，我们还可以自动减。这个和http的prefork模型很像。首先要有一个最小进程，如果用户进来，进程不够了，就会自动增加进程。



万一pod所在的节点宕机了，这个pod就需要被调度到其他节点上重建，而新建的pod和原来的pod不是同一个。他只不过里面运行的是同一个服务，但是pod已经不是那个pod，而且每一个容器都应该有IP地址，新的pod中的ip地址很可能也与原来的那个pod不一样，那么这么一来，就有问题了，我们客户端怎么去访问那些pod呢？比如，我们以前配置lnmp，在前面对应的nginx中，配置好他对应的php的地址是什么，如果我们的php是在pod中，pod的地址发生了改变怎么办，所以此时，我们需要的是服务发现。首先客户端每一次去访问后端对应的服务的时候，我们并不是直接访问后端，而是去找某一个位置发现一下，有没有这种服务。比如，我们要做饭，需要鸡蛋，我们需要去养鸡场去买，但是你怎么知道养鸡场在哪呢？原来的方式呢，是我们就知道养鸡场在哪，我们每次都去那买，后来养鸡场搬迁了，他也没说搬哪去了，就找不到了。常识里，有一个办法，就找一个农贸市场，所有负责生产购置商品的，他们都到这个市场来注册一个摊位。当我们再去买的时候就不用找农场了，我们只需要知道这个市场的地址就行了。于是，任何人想要买东西的时候，就到这个市场上来就行了。当然，我们的服务不能放在市场上去，但是我们可以在市场上注册，说明自己在哪。所以每一次客户端想要访问服务，都来这个市场来问，我要找一个XXX服务，有没有。一旦后端的应用宕机了，我们的市场上有类似于heartbeat的方式，探测每个后端程序还在不在线，如果在指定周期中不在了，就会把他移除了。然后会重试，如果3次访问不到，就认为这个程序宕机了，就把他删除，然后再去注册的商户里面再找一个来提供服务。当一个pod出现故障，我们就需要新建一个pod，这个pod上来的第一件事就是到市场注册一下，让别人找到。而在kubernetes中，有专门这么一种资源，来实现服务发现的。



我们在回到pod这里，pod是有生命周期的。一个pod随时可能被销毁，随时有新的pod生成，他们提供的是同一种服务，我们客户端是没办法通过固定的手段来访问pod的，而pod是不固定的，不论你使用主机名，还是IP地址，他们随时都可能被替换掉，这就需要用到服务发现机制。为了尽可能降低二者协调的复杂度，kubernetes为每一组提供同类服务的pod和他的客户端中间添加了一层中间层，而中间层是固定的，这个中间层就叫做service。service只要不删除，它的地址就是固定的，名称也是固定的，而后当客户端需要访问服务时，在客户端写好服务的地址，而这个服务是一个调度器，不但可以服务发现，还可以调度，能够提供稳定的访问入口。 只要不被删除，入口就不会变，我们客户端只需要访问这个不变的入口，由service吧请求调度到后端的pod上。一旦pod所在节点宕机了，或者由于其他情况消失了，你新建的pod立刻会被service关联进来，作为service后端可用服务之一。客户端访问服务都是靠IP地址和端口或者主机名加端口来实现的。而service关联后端的pod不是靠pod的ip地址实现，pod上有一个固定的metadata，叫标签。只要创建的pod的label是统一的，不管IP地址怎么变，主机名怎么变都能被service识别，因为service是靠标签选择器来关联对象的。所以，这么一来，只要pod存在，只要他属于这个标签选择器，就立即能够被service挑中，并作为service的后端组件。pod被关联进来之后，在动态探测pod的IP地址，端口，并作为后端调度的目标服务器。因此，客户端的请求到达service，由service代理至后端pod。而在k8s上，service不是什么应用程序，也不是一个实体组件，他只不过IPtables的dnet规则。dnet规则上说，所有到达我这个主机的请求，全部被目标地址转换，转发到后端pod。Dnet规则中的IP地址并不会出现在任何一个网卡上，他实际是不存在的， 请求看见的地址，都是service的地址，他ping不同，但是的确可以请求。ping通是因为有tcpip协议栈来响应，但是这里是没法响应的。地址仅出现在规则当中。



更重要的是，service作为kubernetes对象来说，有自己的名称，而service的名称，就是这个服务的名称。名称可以被解析，你可以吧service的名字，直接解析成service的IP。我们装完k8s之后的第一件事，就需要在kuberntes集群上部署一个DNS pod，以确保名称可以被解析，而这种pod是k8s自身就要用的pod。我们称他为基础级的系统架构级的pod，而且他们也被称为集群的附件。叫addons，也叫plugin，叫附件。其实kubernetes的附件中dns只是其中一个，这种dns有一个很有意思的特点，他会动态改变，动态增加，动态删除。怎么变呢，比如：我们把service的名称改一改，他会自动触发dns解析记录中的名称也改变。假如，我们手动把service的dns地址改了，改完会自动触发dns的解析记录也改。 所以以后客户端再去访问某一个服务的时候，可以直接访问服务的名称，由集群中专用的dns服务来负责解析，解析的是service的地址，不是pod地址。因此这个访问是由service代理实现，而这个代理，是端口代理，dnat来实现。但是，service背后是两个或者更多的规则，dnat就意味着多目标，对于多目标调度，linux上的iptables已经把负载均衡的功能主要交给IPVS来实现了。因此，如果service的背后有很多pod，使用dnat规则来实现，在调度效率上可能不太尽如人意，因此在1.11版当中，已经把iptables规则，进一步改成了ipvs规则。实际上，我们每创建一个service，就会生成一个ipvs规则。只不过是net模型的ipvs，所以说lvs算是一个基础性的服务。他是其他很多服务的根本。



![image-20200326232212407](/pages/keynotes/L2_advanced/1_kubernetes/pics/1_Architecture/image-20200326232212407.png)

对于我们有一个数据中心，那么是不是每一台服务器都需要被互联网所访问呢？必然不会，只有nginx这种客户端才需要被外部访问。在集群中的主机，他很有可能既是客户端又是服务端，他即便是想访问其他服务，也是集群内的另外一个服务，而不是集群外的，所以大部分事情，在集群内部就可以解决了，只有极个别的我们才需要开放给集群外部访问。假设我们运行nmt，tomcat的客户端是nginx，tomcat本身是不需要提供给互联网访问的。为什么说这个呢？我们知道，一般机房外面都有防火墙，只把需要被外部访问的服务在防火墙上配置规则，放行流量。假如，我们现在所有的程序都运行在了k8s之上了，我们可以集群想象成一个机房。我们以nmt的形式运行，n作为入口，它的后端可能不只一个pod，想要nginx访问tomcat，就需要在t前面做一个service，所以nginx访问t的时候，是访问t的固定接入层，也就是service。同样，t要访问m，m前面也要加一个service。那么n要做高可用pod，前面也需要做一个service，为n提供一个统一的入口。一个只提供ipvs规则的ip怎么能真正被外部访问到呢，因为这个地址是私有地址。



为了组织成为一个集群，我们就需要一个物理节点，节点自己有地址，让请求先到达物理服务器的地址，由物理服务器再代理转发代理至service，要做外部转发，我们的外部还需要一个调度器，拥有公网地址的。每一个节点都有入口，都可以作为转发节点，我们就需要在集群外部做一个调度器，这个调度器转发请求到节点上，而节点上只能有一个端口来转发作为服务的端口。这样就是三次转发。



如果我们把k8s集群运行在aws上面，这多个ec2构成了一个k8s集群，而后，aws支持lbaas，所以k8s可以调用底层的云计算环境，调用API接口，声明我创建的service需要对外开放，那么就可以触发aws云计算给我们创建一个调度器，将请求接入进来。



我们为了做一个nmp，我们实现了，第一，每个服务都需要有一个service组件，第二，如果服务器当中的pod挂了怎么办，谁来管理？控制器，外部有控制器来创建pod，控制器怎么来选择pod呢，标签和标签选择器。



我们创建pod的过程就是，我们要创建一个pod控制器，控制器会帮我们把pod创建出来。service呢？需要手动创建，在创建完控制器之后，再创建service。而service有两种类型，创建后，只供pod内部所访问，还可以供外部来访问。从本质上来讲，这就是我们k8s的基本组件。



但是每一个service要想基于名称被客户端访问和发现，我们还需要dns服务，这个服务，也是一个pod，那么这个dns是不是也应该有一个service，也有service。



另外每一个pod在运行过程中，用了多少资源，是不是故障了，接受的用户访问，是不是需要被监控呢，比如hipster+grafana，或者是prometheus+grafana，他们也是附件。



这些就是核心组件。



我们在说一个，在k8s中网络模型非常复杂，他要求集群中要有三种网络，第一个，每个pod运行在一个网络中，而service是另外一个网络，service地址和pod地址是不同的网段的，pod的地址是配置在宿主机网络名称空间中的，是可以ping通的，而service的地址是假的，他只存在于iptables或者ipvs的规则当中，我们各个节点是不是也有IP地址，那么就有三个网段，pod，service和node。请求先到达node网络，然后是service网络，最后是pod网络。



那么pod和pod怎么通信呢，在k8s上还存在三类通信，第一，一个pod内可能有多个容器，一个pod内的多个容器怎么通信？lo本地通信。第二，各个pod怎么通信，那么我们来看docker的通信，两个docker容器，在同一个主机上，是通过docker0通信的，在不同的主机呢，他两通信要这么做。请求在出去之前，要做snet，而另外一个pod想要被访问，要先dnet，把pod发布出去，其实他们两个的通信，就是一个两级转换，那么在k8s中，这样是不允许的。他们通讯是通过pod的ip地址通讯的，无论这个pod在哪个节点，他们的地址都不应该冲突。我们有这么几种形式，第一，物理桥桥接，但是这样的话，如果我们一个桥上有300个pod，就需要有300个地址，这还好，因为是处在二层网络，他们要通讯就必须要广播，如果有30000个容器呢。我们还有另外一种方案，叫overlay network，叫叠加网络。我们通过隧道的方式来转发二层报文，使得他两个虽然跨主机，但是好像工作在同一个二层网络中一样，其实，有二层叠加，也有三层叠加。我们可以转发对方的二层报文或者隧道转发对方的三层报文来实现叠加网络。这样一来pod和pod就可以通讯了。 第三个，pod和service通信，刚才说了，pod有pod网络，service有service网络，他们不在同一个网络，怎么直接通信呢？刚才说了service不过是主机上的iptables规则么，所以说，如果创建了service，这个规则就需要反映在每一个节点之上的，如果有一个容器试图去访问service的IP地址时，他应该吧请求送给网关，一般就是docker0桥的地址。而docker0桥收到请求以后，怎么知道service地址呢，他会去检查iptables规则表，我们知道，service随时也有可能变动，你删除了service，创建service，或者service背后的pod改变了，那么他的规则中的目标地址也要改变。因此service随时要变。但是，service怎么能够改变所有节点上的地址规则呢，这就需要一个专门的组件来实现，在每一个node上有一个专门的组件，这个组件是运行在node之上的守护进程，他被称为kube-proxy，他负责与apiserver通信，如果service发生了改变，信息会储存在apiserver中，而apiserver的通信后，内容发生改变，他就会生成一个事件，这个事件可以被任何关联的组件所发现，一旦kube-proxy发现service发生改变，kube-proxy负责在本地吧地址反应在iptables或者lvs规则当中，所以service的管理是靠kube-proxy来实现。而这种动态的管理却不需要人来干预，他的内部有一套逻辑来实现，这么一来，我们就发现api-server是不是需要存储整个集群中的各种信息啊，那他的信息存在哪呢？我们就需要一个共享存储，对于各种信息，api-server并不是吧信息存放在主机之上，而是存在于db之中，这个db叫etcd。而etcd是一个kv存储，和redis很相像，但是etcd本身还具有很多的协调功能，这个是redis不具备的，支持raft协议，支持选举。其实从这个角度来说，etcd更想是zookeeper。如果etcd挂了有什么影响？整个集群就挂了，所以etcd就必须要做高可用，一般必须有三节点。而etcd也是restful风格的，通过http协议来通讯，为了安全，他应该配置成https。而etcd通信也有两个端口，一个作为内部通讯，一个作为对外服务。也就意味着内部通讯需要一个点对点证书，向客户端提供加密，就需要另外一套证书来加密。同样的道理，k8s的apiserver 也是https协议，同样也需要一套证书。而API是不是还要和kubelet通信，也需要证书，和kube-proxy通信，也需要证书。所以我们要去手动部署k8s，最麻烦的就是这5套证书。而这每一套都需要一个独立的CA，而且是双向认证，为了保证安全。其实我们也可以简化，整个集群也不过是三类节点，第一类是api，第二类是etcd，第三类就是node，彼此之间都是https通信。



好了，现在我们就能在node上运行pod了，所以我们还需要构建网络。但是，k8s对于刚才说的三种网络通讯方式并不提供，我们要想运行管理网络，就需要第三方插件。无论是哪一个第三方所提供的解决方案，都至少应该可以管理两种网络，pod网络和集群网络。节点是需要运维来解决的。k8s来通过cni接口来接入外部的网络插件，其实他更像是一个标准，不管哪个软件，只要按照我的接口来开发，就可以接近我的网络方案。事实上，这些个插件都可以作为pod运行在集群之上。这个pod需要共享节点的网络名称空间，就好像docker的一种网络模型，直接共享系统的名称空间，使用系统的网络名称空间的接口作为自己的网络。而目前可以作为CNI网络插件的软件真是太多了。但是目前，常见的是flanel。其实网络插件的功能有两个维度，一个是给pod或者service提供IP地址，另外一个，还需要提供network policy。其实pod和pod之间有了ip地址就可以直接通信，那么为什么还需要网络策略呢？因为容器中的pod总是在变，如果地址变动了怎么办呢？策略就是来解决这个问题的。而不是通信的问题。另外一个关于策略的，就是他能提供网络隔离。当然，隔离也是通过iptables。最后一个逻辑组件就是名称空间，我们可以吧一类pod运行在一个名称空间中，这个空间并不是资源的边界，而是管理的边界，比如：空间1是开发的，空间二是生产的。如果我们要删除空间，那么空间中的所有资源都会被删掉。但是和pod没有边界，而网络策略可以定义不同名称空间或者相同名称空间中的pod可不可以互相访问。这都是通过iptables来实现。对k8s来说，网络策略和网络功能是两个维度的东西，flannel只能实现网络配置，不支持网络策略。calico，有网络配置和网络策略，但是calico部署和使用有点难，而且calico实现的是bgp协议直接的路由通信。于是有出现了另外一种，canel，用flannel来提供网络，用calico来提供策略。flannel是coreos的，calico是独立的项目。而这些pod都可以托管在k8s上运行，也可以独立运行。

三个网络

![image-20200327005654250](/pages/keynotes/L2_advanced/1_kubernetes/pics/1_Architecture/image-20200327005654250.png)



有了这些，我们再看k8s就简单多了。但是我们安装还是会要人命，特别是一些开发的朋友。所以，k8s官方就提供了很多种安装方法，比较典型的，利用ansible的role，kubespray。而官方提供的是kubeadm，这种方式会把所有组件统统运行为容器，除了kubelet。不过这些镜像都托管在gcr当中。还有第三种方式，我们还有一个minikube，这个靠一个节点就可以运行。